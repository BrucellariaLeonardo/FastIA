{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Initialize* the weights.\n",
    "1. For each image, use these weights to *predict* whether it appears to be a 3 or a 7.\n",
    "1. Based on these predictions, calculate how good the model is (its *loss*).\n",
    "1. Calculate the *gradient*, which measures for each weight, how changing that weight would change the loss\n",
    "1. *Step* (that is, change) all the weights based on that calculation.\n",
    "1. Go back to the step 2, and *repeat* the process.\n",
    "1. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don't want to wait any longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_step(params, prn=True):\n",
    "    preds = f(time, params)\n",
    "    loss = mse(preds, speed)\n",
    "    loss.backward()\n",
    "    params.data -= lr * params.grad.data\n",
    "    params.grad = None\n",
    "    if prn: print(loss.item())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primero importemos las librerias de fast ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora importemos el data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/data/mnist_sample\n"
     ]
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que path posee una muestra de minist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/storage/data/mnist_sample/train'),Path('/storage/data/mnist_sample/labels.csv'),Path('/storage/data/mnist_sample/valid')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos el conjunto de validacion, de entrenamiento y un csv con las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorImage (ruta): # recibo una ruta con una serie de imagenes convertir en tensor\n",
    "    return torch.stack([tensor(Image.open(i)) for i in ruta]).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inPuts = torch.cat([ tensorImage(((path/'train/3').ls())), tensorImage(((path/'train/7').ls())) ]).view([-1, 28*28])\n",
    "targets = tensor([0]*len((path/'train/3').ls())+ [1]* len((path/'train/7').ls())).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validSet = torch.cat([ tensorImage(((path/'valid/3').ls())), tensorImage(((path/'valid/7').ls())) ]).view([-1, 28*28])\n",
    "validTargets = tensor([0]*len((path/'train/3').ls())+ [1]* len((path/'train/7').ls())).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un un conjunto de datos, podemos hacer un data loader con ayuda de pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataloader:\n",
    "\n",
    "    def __init__(self, trainSet, trainTargets, validSet, validTargets, batchSize):    \n",
    "        #guardo los data sets emparejados con sus respectivas etiquetas de salida\n",
    "        self.batchSize = batchSize\n",
    "        self.trainSet = self.zipSet(trainSet, trainTargets)\n",
    "        self.validSet = self.zipSet(validSet, validTargets)\n",
    "        self.trainBatches = MyDataloader.genBatches(self.trainSet, batchSize)\n",
    "        self.validBatches = MyDataloader.genBatches(self.validSet, batchSize)\n",
    "    def zipSet (self, inPut, target):\n",
    "        #Retorna una lista de duplas a partir un tensor de inputs y un tensor de targets asociados a dicho input\n",
    "        duplas= zip(inPut, target)\n",
    "        return list(duplas)\n",
    "    \n",
    "    def genBatches ( Dset, batchSize):\n",
    "        #recibe una lista y un tamaño de lote, para retornar una lista con sub listas de la lista recibida del tamaño indicdo\n",
    "        batches = []                   #armo una lista vacia donde guardar los distintos lotes\n",
    "        nElements = len(Dset)          #miro cuantos elementos tengo que randomizar\n",
    "        index = th.randperm(nElements)      #genero una lista de indices randomizados\n",
    "        nBatches = int(nElements/batchSize)\n",
    "        for i in range (nBatches):\n",
    "            batchAux = Dset[batchSize*i : batchSize*(i+1)]\n",
    "            batches.append(batchAux)\n",
    "        #la conversion a int nos dejo fuera de la iteracion n elementos que son el resto de la divicion para nBatches\n",
    "        if(nElements % batchSize != 0): #si mi divicion de lotes no es exacta\n",
    "            batches.append(Dset[batchSize*(i+1) : -1]) # agrego un ultimo lote con los elementos faltantes\n",
    "        return batches\n",
    "    \n",
    "    #para acceso a los batches = [numero de batch] [numero de dupla input/target] [elemento de la dupla ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dl = MyDataloader(inPuts, targets, validSet, validTargets, 1000)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de nuestra red, basicamente se reduce a los parametros/pesos (weights) de las neuronas, a los baias y a las forma de calcular lass predicciones, osea como usar los pesos y los baias. Implementemos un modelo de 1 capa que reciba un una cantidad de neuronas para la capa y posea un metodo para predecir con dichas neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelo:\n",
    "    \n",
    "    def __init__(self, nNeuronas):\n",
    "        self.wights = torch.randn(nNeuronas).requires_grad_()\n",
    "        self.bias = torch.randn(1).requires_grad_()\n",
    "    \n",
    "    def predict(self, inPuts):\n",
    "        return th.sigmoid((inPuts @ self.wights).sum() + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Modelo(28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6518], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.predict(dl.trainBatches[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora obtengamos el loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos nuestro modelo, necesitamos una forma de medir que tan bien o mal esta nuestr prediccion, el loss, la cual es simplemente una funcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El optimizer es el encargado de realizar la actualizacion de los parametros del modelo, mediante la backpropagation generada a partir del aclculo del loss. por lo que sera una funcion que necesitara de los parametros del modelo y el resultado del loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer (params, bias, loss, lr):\n",
    "    loss.backward()           #calcula el gradiente a partir de loss\n",
    "    params.data = params.data - params.grad * lr #actualizar params y bias\n",
    "    bias.data = bias.data - bias.grad * lr\n",
    "    params.grad.zero_() # restablece los gradientes\n",
    "    bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definamos la metrica (error cuadratico medio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errCuad (pred, target):\n",
    "    return ((pred - target)**2).mean(-1,-2).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un learner debe de poder contener, un conjunto de datos, los parametros del modelo, la metrica, el loss y el optimizer. Y luego debe brindar la funcionalidad de entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
